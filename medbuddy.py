# -*- coding: utf-8 -*-
"""MedBuddy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MpObv1iKL6cQxvxD6u8laaCFzoynGGhS
"""

!pip install langchain_groq langchain_core langchain_community

from langchain_groq import ChatGroq
llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
result = llm.invoke("What is mental well-being")
print(result.content)

!pip install pypdf

!pip install chromadb

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os

def initialize_llm():
  llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
  return llm

def create_vector_db():
  loader = DirectoryLoader("/content/data/", glob = '*.pdf', loader_cls = PyPDFLoader)
  documents = loader.load()
  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)
  texts = text_splitter.split_documents(documents)
  embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')
  vector_db = Chroma.from_documents(texts, embeddings, persist_directory = './chroma_db')
  vector_db.persist()

  print("ChromaDB created and data saved")

  return vector_db

def setup_qa_chain(vector_db, llm):
  retriever = vector_db.as_retriever()
  prompt_templates = """ You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
    {context}
    User: {question}
    Chatbot: """
  PROMPT = PromptTemplate(template = prompt_templates, input_variables = ['context', 'question'])

  qa_chain = RetrievalQA.from_chain_type(
      llm = llm,
      chain_type = "stuff",
      retriever = retriever,
      chain_type_kwargs = {"prompt": PROMPT}
  )
  return qa_chain

def main():
  print("Intializing Chatbot.........")
  llm = initialize_llm()

  db_path = "/content/chroma_db"

  if not os.path.exists(db_path):
    vector_db  = create_vector_db()
  else:
    embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
  qa_chain = setup_qa_chain(vector_db, llm)

  while True:
    query = input("\nHuman: ")
    if query.lower()  == "exit":
      print("Chatbot: Take Care of yourself, Goodbye!")
      break
    response = qa_chain.run(query)
    print(f"Chatbot: {response}")

if __name__ == "__main__":
  main()

create_vector_db()

!pip install gradio

from langchain_community.embeddings import HuggingFaceBgeEmbeddings  # Correct import
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader  # Correct import
from langchain_chroma import Chroma  # Correct import for Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq  # Missing import
import os
import gradio as gr

def initialize_llm():
    # Consider using environment variables for API key
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",  # Move to .env
        model_name="llama-3.3-70b-versatile"
    )
    return llm

def create_vector_db():
    loader = DirectoryLoader("/content/data/", glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')
    vector_db.persist()

    print("ChromaDB created and data saved")
    return vector_db

def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_template = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
    Context: {context}
    User: {question}
    Chatbot: """

    PROMPT = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True  # Good for debugging
    )
    return qa_chain

print("Initializing Chatbot.........")
llm = initialize_llm()

db_path = "./chroma_db"  # Fixed path

if not os.path.exists(db_path):
    print("Creating new vector database...")
    vector_db = create_vector_db()
else:
    print("Loading existing vector database...")
    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

qa_chain = setup_qa_chain(vector_db, llm)

def chatbot_response(user_input, history):
    if not user_input.strip():
        return "Please provide a valid input", history

    try:
        # Use invoke instead of run for better error handling
        result = qa_chain.invoke({"query": user_input})
        response = result["result"]

        # Add to history
        history.append((user_input, response))
        return "", history

    except Exception as e:
        error_msg = f"Sorry, I encountered an error: {str(e)}"
        history.append((user_input, error_msg))
        return "", history

# Create Gradio interface
with gr.Blocks(theme=gr.themes.Soft()) as app:  # Using built-in theme
    gr.Markdown("# üß† Mental Health Chatbot ü§ñ")
    gr.Markdown("A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.")

    chatbot = gr.Chatbot(label="Conversation")
    msg = gr.Textbox(label="Your message")
    clear = gr.Button("Clear")

    def respond(message, chat_history):
        if not message.strip():
            return "", chat_history
        result = qa_chain.invoke({"query": message})
        chat_history.append((message, result["result"]))
        return "", chat_history

    msg.submit(respond, [msg, chatbot], [msg, chatbot])
    clear.click(lambda: None, None, chatbot, queue=False)

    gr.Markdown("### ‚ö†Ô∏è Important Notice")
    gr.Markdown("This chatbot provides general support. For urgent issues, seek help from licensed professionals.")

if __name__ == "__main__":
    app.launch(share=True)  # Added share=True for external access

!pip install langchain_chroma